{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on Brazilian E-Commerce Public Dataset by Olist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_customers_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing spark session\n",
    "spark = SparkSession.builder.appName(\"FinalProject\")\\\n",
    "        .config('spark.driver.extraClassPath','/usr/lib/jvm/java-11-openjdk-amd64/lib/postgresql-42.6.0.jar')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your YAML file\n",
    "yaml_file_path = 'config.yaml'\n",
    "\n",
    "# Read the YAML file and parse it into a Python dictionary\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['DB_URL'] = config['database']['url']\n",
    "os.environ['DB_URL_SAVE'] = config['database']['url_save']\n",
    "os.environ['DB_DRIVER'] = config['database']['driver']\n",
    "os.environ['DB_USER'] = config['database']['user']\n",
    "os.environ['DB_PASSWORD'] = config['database']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use environment variables for database connection\n",
    "db_url = os.environ['DB_URL']\n",
    "db_url_save = os.environ['DB_URL_SAVE']\n",
    "db_driver = os.environ['DB_DRIVER']\n",
    "db_user = os.environ['DB_USER']\n",
    "db_password = os.environ['DB_PASSWORD']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining path to the dataset\n",
    "customer_data_path = \"./Data/olist_customers_dataset.csv\"  \n",
    "order_item_path = \"./Data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"./Data/olist_order_payments_dataset.csv\"\n",
    "product_category_translation_path= \"./Data/product_category_name_translation.csv\"\n",
    "product_path = './Data/olist_products_dataset.csv'\n",
    "seller_path = './Data/olist_sellers_dataset.csv'\n",
    "orders_path = './Data/olist_orders_dataset.csv'\n",
    "review_path = \"Data/reviews_translated.csv\"  \n",
    "\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "customer_df = spark.read.csv(customer_data_path, header=True, inferSchema=True)\n",
    "order_item_df = spark.read.csv(order_item_path, header=True, inferSchema=True)\n",
    "order_payment_df = spark.read.csv(order_payment_path, header=True, inferSchema=True)\n",
    "product_category_translation_df = spark.read.csv(product_category_translation_path, header=True, inferSchema=True)\n",
    "seller_df_uncleaned = spark.read.csv(seller_path, header=True, inferSchema=True)\n",
    "product_df_uncleaned = spark.read.csv(product_path, header=True, inferSchema=True)\n",
    "orders_df = spark.read.csv(orders_path, header=True, inferSchema= True)\n",
    "reviews_df = spark.read.csv(review_path, header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING WHITESPACE\n",
    "\n",
    "# Remove leading and trailing whitespace from all columns\n",
    "seller_df_uncleaned.select([f.trim(f.col(c)).alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "# Remove whitespace characters between words in all columns\n",
    "seller_df = seller_df_uncleaned.select([f.regexp_replace(f.col(c), r'\\s+', ' ').alias(c) for c in seller_df_uncleaned.columns])\n",
    "\n",
    "\n",
    "#Replacing column on product dataset with content from product category translation dataset\n",
    "\n",
    "# left join between the 'product_df_uncleaned' DataFrame and 'product_category_translation_df'\n",
    "product_joined_df= product_df_uncleaned.join(product_category_translation_df, \"Product_category_name\", \"left\")\n",
    "\n",
    "# Drop \"product_category_name\" will be removed from the DataFrame.\n",
    "product_df = product_joined_df.drop(\"product_category_name\")\n",
    "\n",
    "# Rename the \"product_category_name_english\" column to \"product_category_name\"\n",
    "product_df = product_df.withColumnRenamed(\"product_category_name_english\", \"product_category_name\")\n",
    "\n",
    "# Replace underscores with spaces in the \"product_category_name\" column\n",
    "product_df = product_df.withColumn(\"product_category_name\", f.regexp_replace(f.col(\"product_category_name\"), \"_\", \" \"))\n",
    "\n",
    "\n",
    "#Defining 0 for not_defined payment\n",
    "\n",
    "# Set payment_installment to 0 where payment_type is \"not_defined\"\n",
    "order_payment_df = order_payment_df.withColumn(\"Payment_installments\",\n",
    "                                   f.when(f.col(\"Payment_type\") == \"not_defined\", 0)\n",
    "                                   .otherwise(f.col(\"Payment_installments\")))\n",
    "\n",
    "\n",
    "# Additional cleaning ON REVIEWS\n",
    "\n",
    "# Casting review_score to integer \n",
    "reviews_df=reviews_df.withColumn(\"review_score\", reviews_df[\"review_score\"].cast(\"int\"))\n",
    "\n",
    "# Replace 'reviews_df' with your actual DataFrame name\n",
    "reviews_df = reviews_df.withColumn(\"review_comment_title\", f.coalesce(f.col(\"review_comment_title\"), f.lit(\"no comment\")))\n",
    "reviews_df = reviews_df.withColumn(\"review_comment_message\", f.coalesce(f.col(\"review_comment_message\"), f.lit(\"no comment\")))\n",
    "\n",
    "# Dropping null values\n",
    "reviews_df =reviews_df.na.drop()\n",
    "\n",
    "# Dropping s_no column\n",
    "reviews_df=reviews_df.drop(\"s_no\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing in parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df.coalesce(1).write.parquet(\"data_cleaned/customer.parquet\",compression =\"snappy\", mode=\"overwrite\") \n",
    "order_item_df.coalesce(1).write.parquet(\"./data_cleaned/order_item.parquet\",compression =\"snappy\", mode=\"overwrite\")\n",
    "order_payment_df.coalesce(1).write.parquet(\"./data_cleaned/order_payment.parquet\",compression =\"snappy\", mode=\"overwrite\")\n",
    "seller_df.coalesce(1).write.parquet(\"./data_cleaned/seller.parquet\",compression =\"snappy\", mode=\"overwrite\")\n",
    "product_df.coalesce(1).write.parquet(\"./data_cleaned/product.parquet\",compression =\"snappy\", mode=\"overwrite\")\n",
    "orders_df.coalesce(1).write.parquet(\"./data_cleaned/orders.parquet\",compression =\"snappy\", mode=\"overwrite\")\n",
    "reviews_df.coalesce(1).write.parquet(\"./data_cleaned/reviews.parquet\",compression =\"snappy\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the clean Dataframes to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'customer', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "order_item_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'order_item', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "order_payment_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'order_payment', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "seller_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'seller', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "product_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'product', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "orders_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'orders', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "reviews_df.write.format('jdbc').options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'reviews', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = spark.read.format(\"jdbc\").options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'reviews', \n",
    "                                user=db_user, \n",
    "                                password=db_password).load()\n",
    "\n",
    "orders_df = spark.read.format(\"jdbc\").options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'orders', \n",
    "                                user=db_user, \n",
    "                                password=db_password).load()\n",
    "\n",
    "order_item_df = spark.read.format(\"jdbc\").options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'order_item', \n",
    "                                user=db_user, \n",
    "                                password=db_password).load()\n",
    "\n",
    "\n",
    "product_df = spark.read.format(\"jdbc\").options(url=db_url,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'product', \n",
    "                                user=db_user, \n",
    "                                password=db_password).load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delivery trend and purphase trend on a day: Compute the delivery deviation in days and actual delivery time also find the order purchase hour in category of Dawn, Morning, Afternoon, Night: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Delivery Time (in days): 12.558216720223907\n",
      "+--------------------+------------------------+-----------------------------+--------------------------+------------------+------------------------+-------------------+\n",
      "|            order_id|order_purchase_timestamp|order_delivered_customer_date|delivery_deviation_in_days|delivery_time_days|Order_purchase_time_slot|Order_purchase_hour|\n",
      "+--------------------+------------------------+-----------------------------+--------------------------+------------------+------------------------+-------------------+\n",
      "|a4de5e12d0e969340...|     2018-06-15 15:44:21|          2018-06-25 22:27:56|                    15.064|             10.28|               Afternoon|                 15|\n",
      "|142721e620ef263e3...|     2017-11-24 13:35:51|          2017-12-05 16:14:56|                    13.323|             11.11|               Afternoon|                 13|\n",
      "|93dbcae5a986d0f71...|     2017-03-18 18:44:50|          2017-03-29 17:28:39|                     7.272|            10.947|               Afternoon|                 18|\n",
      "|39a82394b2cec1398...|     2017-01-30 23:17:07|          2017-02-17 09:05:13|                    23.621|            17.408|                   Night|                 23|\n",
      "|b7293e3014a7261f0...|     2017-10-30 14:27:30|          2017-11-08 19:41:15|                     18.18|             9.218|               Afternoon|                 14|\n",
      "|35f741bc89012d159...|     2018-02-25 18:06:16|          2018-03-29 01:20:50|                   -13.056|            31.302|               Afternoon|                 18|\n",
      "|6a106cdfc41258737...|     2018-01-03 17:21:39|          2018-01-15 13:46:43|                     3.426|            11.851|               Afternoon|                 17|\n",
      "|42e1c1e283ac7a8da...|     2018-05-10 14:31:13|          2018-05-22 13:47:40|                    19.425|             11.97|               Afternoon|                 14|\n",
      "|b085981e816dad298...|     2017-10-18 23:23:11|          2017-10-27 16:42:33|                    12.304|             8.722|                   Night|                 23|\n",
      "|ad5425b24e9a5cade...|     2017-11-21 21:20:41|          2017-11-25 16:37:47|                     5.307|             3.804|                   Night|                 21|\n",
      "|4336f383d6d1f9c9d...|     2018-02-12 19:12:37|          2018-02-22 12:08:57|                     3.494|             9.706|                   Night|                 19|\n",
      "|e71929c560756149b...|     2017-12-18 10:20:57|          2017-12-22 23:10:05|                    13.035|             4.534|                 Morning|                 10|\n",
      "|95f4e9b4497d4252f...|     2018-02-19 09:45:59|          2018-02-22 20:44:10|                    10.136|             3.457|                 Morning|                  9|\n",
      "|930ce01cc6b66e958...|     2018-05-03 16:55:44|          2018-05-14 16:44:46|                    24.302|            10.992|               Afternoon|                 16|\n",
      "|a477cdebba202e2c1...|     2017-11-23 21:27:36|          2017-12-08 10:51:39|                     2.547|            14.558|                   Night|                 21|\n",
      "|2d0f32fa8bc788551...|     2018-05-22 14:50:39|          2018-05-30 16:29:33|                     7.313|             8.069|               Afternoon|                 14|\n",
      "|2a5e2ec255be0a5cb...|     2017-10-14 15:03:34|          2017-11-16 17:09:52|                    -3.715|            33.088|               Afternoon|                 15|\n",
      "|4884b6f84bbec8bdf...|     2018-02-23 12:25:56|          2018-03-06 16:33:45|                     16.31|            11.172|                 Morning|                 12|\n",
      "|c0b4e1cd8410a7ec7...|     2018-02-16 18:19:00|          2018-03-02 18:09:04|                     5.244|            13.993|               Afternoon|                 18|\n",
      "|946a2b3561cd706b8...|     2018-07-07 14:00:19|          2018-07-13 18:46:45|                    17.218|             6.199|               Afternoon|                 14|\n",
      "+--------------------+------------------------+-----------------------------+--------------------------+------------------+------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter delivered orders only\n",
    "delivered_orders_df = orders_df.filter(orders_df[\"Order_status\"] == \"delivered\")\n",
    "\n",
    "# Extract the hour of the day from the timestamp\n",
    "delivered_orders_df = delivered_orders_df.withColumn(\"Order_purchase_hour\", \n",
    "                                                     f.hour(\"Order_purchase_timestamp\"))\n",
    "\n",
    "# Calculate delivery time in days by finding the time difference between order delivered and purchase timestamps\n",
    "delivered_orders_df = delivered_orders_df.withColumn(\n",
    "    \"delivery_time_days\",\n",
    "    f.round((f.unix_timestamp(\"order_delivered_customer_date\") - f.unix_timestamp(\"order_purchase_timestamp\")) / (24 * 3600),3)\n",
    ")\n",
    "\n",
    "# Calculate delivery deviation in days by finding the time difference between estimated delivery date and actual delivery date\n",
    "delivered_orders_df = delivered_orders_df.withColumn(\n",
    "    \"delivery_deviation_in_days\",\n",
    "    f.round((f.unix_timestamp(\"order_estimated_delivery_date\") - f.unix_timestamp(\"order_delivered_customer_date\")) / (24 * 3600),3)\n",
    ")\n",
    "\n",
    "# Calculate average delivery time \n",
    "average_delivery_time = delivered_orders_df.selectExpr(\"avg(delivery_time_days) as avg_delivery_time\").first()[\"avg_delivery_time\"]\n",
    "print(\"Average Delivery Time (in days):\", average_delivery_time)\n",
    "\n",
    "# Create a column for the time slot with its category of purchase hour\n",
    "delivered_orders_df = delivered_orders_df.withColumn(\"Order_purchase_time_slot\",\n",
    "    f.when((delivered_orders_df[\"Order_purchase_hour\"] >= 0) & (delivered_orders_df[\"Order_purchase_hour\"] <= 6), \"Dawn\")\n",
    "    .when((delivered_orders_df[\"Order_purchase_hour\"] >= 7) & (delivered_orders_df[\"Order_purchase_hour\"] <= 12), \"Morning\")\n",
    "    .when((delivered_orders_df[\"Order_purchase_hour\"] >= 13) & (delivered_orders_df[\"Order_purchase_hour\"] <= 18), \"Afternoon\")\n",
    "    .otherwise(\"Night\")\n",
    ")\n",
    "\n",
    "# Selecting columns for final result\n",
    "delivery_status = delivered_orders_df.select(\"order_id\", \"order_purchase_timestamp\", \"order_delivered_customer_date\",\"delivery_deviation_in_days\",\n",
    "                                             \"delivery_time_days\",\"Order_purchase_time_slot\",\"Order_purchase_hour\")\n",
    "\n",
    "# Showing the final result\n",
    "delivery_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the review sentiment analysis and the coorelation between sentiment score and review score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        Correlation|\n",
      "+-------------------+\n",
      "|0.23821213214691553|\n",
      "+-------------------+\n",
      "\n",
      "Correlation between sentiment_score and review_score: 0.23821213214691553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------------------+---------------------+---------------+\n",
      "|            order_id|           review_id|review_score|review_comment_message|product_category_name|sentiment_score|\n",
      "+--------------------+--------------------+------------+----------------------+---------------------+---------------+\n",
      "|bfc318d0ca556b5d3...|a5db6f896a16fc556...|           1|  because it is ano...|       sports leisure|           -1.0|\n",
      "|67a41e5f40c8ee15e...|05809c068ac8f403a...|           1|  I made the purcha...|        health beauty|           -1.0|\n",
      "|98aee9cbf5501420f...|0eb547125304a7c10...|           1|  I bought a cartri...| computers accesso...|           -1.0|\n",
      "|8c3078b30ae073033...|6bc92786a5bd0870c...|           1|              Horrible|            telephony|           -1.0|\n",
      "|13832510afc6f9410...|8cecdf29b454546b6...|           1|  I did not receive...|                 null|           -1.0|\n",
      "|68810c6b108472c39...|12b47bc6b93e0e84e...|           1|  The terrible work...|        watches gifts|           -1.0|\n",
      "|03a8aa5d0860898d4...|de181f574f313b118...|           1|  Terrible, I'm try...|                 auto|           -1.0|\n",
      "|cfa78b997e329a529...|8ea780895f362ce94...|           1|  It is not a mini ...|  musical instruments|           -1.0|\n",
      "|48d5f41593d593945...|f8f0539fdc3209924...|           1|  Simply horrible p...|       bed bath table|           -1.0|\n",
      "|fc74153e0ac39bb68...|237ec2d8031fa4fc3...|           1|  Terrible attendance!|            perfumery|           -1.0|\n",
      "|5a1c2be64df8ac1e3...|ab37099d60debc34f...|           2|  Terrible I'm wait...|       bed bath table|           -1.0|\n",
      "|1d52ba7197c7acebb...|18825455b8639598b...|           1|  I did not receive...| computers accesso...|           -1.0|\n",
      "|5850f6bb95a78cd0f...|005eba8f7c09057f6...|           1|  Horrible store, d...|                 toys|           -1.0|\n",
      "|4130a3c17d5018e88...|5bb70f8c3ba56c5f3...|           1|  I never buy from ...|      furniture decor|           -1.0|\n",
      "|83c76350df165cbfe...|c7b17b45bfd738f34...|           1|  Horrible product ...|           cool stuff|           -1.0|\n",
      "|e7e206c19879bfaf6...|69074582b3f4c2075...|           1|  I didn't like the...|        health beauty|           -1.0|\n",
      "|1dd7e8d1a469f3ce2...|748bceb2d47f38966...|           1|  I did not receive...|      fixed telephony|           -1.0|\n",
      "|6fa25e5deedf726d2...|38b57347a0c958b44...|           4|  I INSTALLED IT IN...| construction tool...|           -1.0|\n",
      "|c71ee8376af932d72...|fa598a2cbeb3e9e49...|           2|  Came missing 1 ki...|      furniture decor|           -1.0|\n",
      "|6fa25e5deedf726d2...|38b57347a0c958b44...|           4|  I INSTALLED IT IN...| construction tool...|           -1.0|\n",
      "+--------------------+--------------------+------------+----------------------+---------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining a UDF for sentiment analysis\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    '''\n",
    "     Analyzes the sentiment polarity of the given text using the TextBlob library.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text for sentiment analysis.\n",
    "\n",
    "    Returns:\n",
    "    float or None: The sentiment polarity score in the range of -1 to 1, where -1 represents\n",
    "    a negative sentiment, 1 represents a positive sentiment, and 0 represents neutral sentiment.\n",
    "    If the input text is \"no comment,\" returns 0.\n",
    "    '''\n",
    "    if text.lower() != \"no comment\":\n",
    "        analysis = TextBlob(text)\n",
    "        polarity = analysis.sentiment.polarity\n",
    "        return polarity\n",
    "    return 0  \n",
    "\n",
    "# Create a User-Defined Function (UDF) to analyze sentiment\n",
    "sentiment_udf = f.udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Calculate sentiment scores by passing the reviews to UDF\n",
    "reviews_df = reviews_df.withColumn(\"sentiment_score\", sentiment_udf(reviews_df[\"review_comment_message\"]))\n",
    "\n",
    "# Cating sentiment_score to float\n",
    "reviews_df=reviews_df.withColumn(\"sentiment_score\", reviews_df[\"sentiment_score\"].cast(\"float\"))\n",
    "\n",
    "# Calculate the correlation between 'sentiment_score' and 'review_score' columns\n",
    "correlation = reviews_df.select(f.corr(\"sentiment_score\", \"review_score\")).first()[0]\n",
    "\n",
    "# Create a DataFrame to store the correlation result\n",
    "correlation_df = spark.createDataFrame([(correlation,)], [\"Correlation\"])\n",
    "\n",
    "# Show the correlation result\n",
    "correlation_df.show()\n",
    "print(\"Correlation between sentiment_score and review_score:\", correlation)\n",
    "\n",
    "#preparing final display output\n",
    "# Joining reviews with order_item and product to get product_category name\n",
    "results_df = reviews_df.join(order_item_df, \"order_id\", \"inner\")\n",
    "joined_df= results_df.join(product_df, on = \"product_id\", how =\"inner\")\n",
    "\n",
    "# Final selection for output display orderby sentiment score\n",
    "joined_df = joined_df.select(\"order_id\", \"review_id\",\"review_score\", \"review_comment_message\",\"product_category_name\",\"sentiment_score\").orderBy(\"sentiment_score\")\n",
    "\n",
    "# Showing final output\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing final results in postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delivery_status.write.format('jdbc').options(url=db_url_save,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'delivery_status_table', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "\n",
    "joined_df.write.format('jdbc').options(url=db_url_save,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'review_sentiment_analysis', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n",
    "\n",
    "correlation_df.write.format('jdbc').options(url=db_url_save,\n",
    "                                driver = db_driver,\n",
    "                                dbtable = 'correlation_of_reviewscore_and_sentiment', \n",
    "                                user=db_user, \n",
    "                                password=db_password).mode('overwrite').save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_1",
   "language": "python",
   "name": ".venv_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
